{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edef9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9de8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99f0ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33e0174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df.drop([\"Survived\"], axis = \"columns\")\n",
    "y_df = df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aad4cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = x_df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b89b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0       3    male  22.0      1      0   7.2500        S\n",
      "1       1  female  38.0      1      0  71.2833        C\n",
      "2       3  female  26.0      0      0   7.9250        S\n",
      "3       1  female  35.0      1      0  53.1000        S\n",
      "4       3    male  35.0      0      0   8.0500        S\n"
     ]
    }
   ],
   "source": [
    "print(x_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cbc93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"Age\"] = x_df[\"Age\"].fillna(x_df[\"Age\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6489c49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df[\"Pclass\"].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "610da268",
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = pd.get_dummies(x_df[\"Pclass\"], drop_first = True, prefix = \"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4091349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass_2  Pclass_3\n",
      "0         0         1\n",
      "1         0         0\n",
      "2         0         1\n",
      "3         0         0\n",
      "4         0         1\n"
     ]
    }
   ],
   "source": [
    "print(dum.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13259f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.concat([x_df,dum], axis = \"columns\")\n",
    "x_df = x_df.drop([\"Pclass\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77bd2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex   Age  SibSp  Parch     Fare Embarked  Pclass_2  Pclass_3\n",
      "0    male  22.0      1      0   7.2500        S         0         1\n",
      "1  female  38.0      1      0  71.2833        C         0         0\n",
      "2  female  26.0      0      0   7.9250        S         0         1\n",
      "3  female  35.0      1      0  53.1000        S         0         0\n",
      "4    male  35.0      0      0   8.0500        S         0         1\n"
     ]
    }
   ],
   "source": [
    "print(x_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cb7ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "g = pd.DataFrame(le.fit_transform(x_df[\"Sex\"]), columns = [\"Sex\"])\n",
    "x_df = x_df.drop([\"Sex\"], axis = \"columns\")\n",
    "x_df = pd.concat([x_df, g], axis = \"columns\")\n",
    "#Male -> 1 Female -> 0\n",
    "\n",
    "e = pd.DataFrame(le.fit_transform(x_df[\"Embarked\"]), columns = [\"Embarked\"])\n",
    "x_df = x_df.drop([\"Embarked\"], axis = \"columns\")\n",
    "x_df = pd.concat([x_df, e], axis = \"columns\")\n",
    "#0->C 1->Q 2->S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03238a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  SibSp  Parch     Fare  Pclass_2  Pclass_3  Sex  Embarked\n",
       "0  22.0      1      0   7.2500         0         1    1         2\n",
       "1  38.0      1      0  71.2833         0         0    0         0\n",
       "2  26.0      0      0   7.9250         0         1    0         2\n",
       "3  35.0      1      0  53.1000         0         0    0         2\n",
       "4  35.0      0      0   8.0500         0         1    1         2\n",
       "5  28.0      0      0   8.4583         0         1    1         1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7f9af4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  SibSp  Parch     Fare  Pclass_2  Pclass_3  Sex  Embarked_Q  \\\n",
      "0  22.0      1      0   7.2500         0         1    1           0   \n",
      "1  38.0      1      0  71.2833         0         0    0           0   \n",
      "2  26.0      0      0   7.9250         0         1    0           0   \n",
      "3  35.0      1      0  53.1000         0         0    0           0   \n",
      "4  35.0      0      0   8.0500         0         1    1           0   \n",
      "\n",
      "   Embarked_S  \n",
      "0           1  \n",
      "1           0  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "dum2 = pd.get_dummies(df[\"Embarked\"], drop_first = True, prefix = \"Embarked\")\n",
    "x_df = pd.concat([x_df, dum2], axis = \"columns\")\n",
    "x_df = x_df.drop(\"Embarked\", axis = \"columns\")\n",
    "\n",
    "print(x_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49f0b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.   38.   26.   35.   35.   28.   54.    2.   27.   14.    4.   58.\n",
      " 20.   39.   14.   55.    2.   28.   31.   28.   35.   34.   15.   28.\n",
      "  8.   38.   28.   19.   28.   28.   40.   28.   28.   66.   28.   42.\n",
      " 28.   21.   18.   14.   40.   27.   28.    3.   19.   28.   28.   28.\n",
      " 28.   18.    7.   21.   49.   29.   65.   28.   21.   28.5   5.   11.\n",
      " 22.   38.   45.    4.   28.   28.   29.   19.   17.   26.   32.   16.\n",
      " 21.   26.   32.   25.   28.   28.    0.83 30.   22.   29.   28.   28.\n",
      " 17.   33.   16.   28.   23.   24.   29.   20.   46.   26.   59.   28.\n",
      " 71.   23.   34.   34.   28.   28.   21.   33.   37.   28.   21.   28.\n",
      " 38.   28.   47.   14.5  22.   20.   17.   21.   70.5  29.   24.    2.\n",
      " 21.   28.   32.5  32.5  54.   12.   28.   24.   28.   45.   33.   20.\n",
      " 47.   29.   25.   23.   19.   37.   16.   24.   28.   22.   24.   19.\n",
      " 18.   19.   27.    9.   36.5  42.   51.   22.   55.5  40.5  28.   51.\n",
      " 16.   30.   28.   28.   44.   40.   26.   17.    1.    9.   28.   45.\n",
      " 28.   28.   61.    4.    1.   21.   56.   18.   28.   50.   30.   36.\n",
      " 28.   28.    9.    1.    4.   28.   28.   45.   40.   36.   32.   19.\n",
      " 19.    3.   44.   58.   28.   42.   28.   24.   28.   28.   34.   45.5\n",
      " 18.    2.   32.   26.   16.   40.   24.   35.   22.   30.   28.   31.\n",
      " 27.   42.   32.   30.   16.   27.   51.   28.   38.   22.   19.   20.5\n",
      " 18.   28.   35.   29.   59.    5.   24.   28.   44.    8.   19.   33.\n",
      " 28.   28.   29.   22.   30.   44.   25.   24.   37.   54.   28.   29.\n",
      " 62.   30.   41.   29.   28.   30.   35.   50.   28.    3.   52.   40.\n",
      " 28.   36.   16.   25.   58.   35.   28.   25.   41.   37.   28.   63.\n",
      " 45.   28.    7.   35.   65.   28.   16.   19.   28.   33.   30.   22.\n",
      " 42.   22.   26.   19.   36.   24.   24.   28.   23.5   2.   28.   50.\n",
      " 28.   28.   19.   28.   28.    0.92 28.   17.   30.   30.   24.   18.\n",
      " 26.   28.   43.   26.   24.   54.   31.   40.   22.   27.   30.   22.\n",
      " 28.   36.   61.   36.   31.   16.   28.   45.5  38.   16.   28.   28.\n",
      " 29.   41.   45.   45.    2.   24.   28.   25.   36.   24.   40.   28.\n",
      "  3.   42.   23.   28.   15.   25.   28.   28.   22.   38.   28.   28.\n",
      " 40.   29.   45.   35.   28.   30.   60.   28.   28.   24.   25.   18.\n",
      " 19.   22.    3.   28.   22.   27.   20.   19.   42.    1.   32.   35.\n",
      " 28.   18.    1.   36.   28.   17.   36.   21.   28.   23.   24.   22.\n",
      " 31.   46.   23.   28.   39.   26.   21.   28.   20.   34.   51.    3.\n",
      " 21.   28.   28.   28.   33.   28.   44.   28.   34.   18.   30.   10.\n",
      " 28.   21.   29.   28.   18.   28.   28.   19.   28.   32.   28.   28.\n",
      " 42.   17.   50.   14.   21.   24.   64.   31.   45.   20.   25.   28.\n",
      " 28.    4.   13.   34.    5.   52.   36.   28.   30.   49.   28.   29.\n",
      " 65.   28.   50.   28.   48.   34.   47.   48.   28.   38.   28.   56.\n",
      " 28.    0.75 28.   38.   33.   23.   22.   28.   34.   29.   22.    2.\n",
      "  9.   28.   50.   63.   25.   28.   35.   58.   30.    9.   28.   21.\n",
      " 55.   71.   21.   28.   54.   28.   25.   24.   17.   21.   28.   37.\n",
      " 16.   18.   33.   28.   28.   26.   29.   28.   36.   54.   24.   47.\n",
      " 34.   28.   36.   32.   30.   22.   28.   44.   28.   40.5  50.   28.\n",
      " 39.   23.    2.   28.   17.   28.   30.    7.   45.   30.   28.   22.\n",
      " 36.    9.   11.   32.   50.   64.   19.   28.   33.    8.   17.   27.\n",
      " 28.   22.   22.   62.   48.   28.   39.   36.   28.   40.   28.   28.\n",
      " 28.   24.   19.   29.   28.   32.   62.   53.   36.   28.   16.   19.\n",
      " 34.   39.   28.   32.   25.   39.   54.   36.   28.   18.   47.   60.\n",
      " 22.   28.   35.   52.   47.   28.   37.   36.   28.   49.   28.   49.\n",
      " 24.   28.   28.   44.   35.   36.   30.   27.   22.   40.   39.   28.\n",
      " 28.   28.   35.   24.   34.   26.    4.   26.   27.   42.   20.   21.\n",
      " 21.   61.   57.   21.   26.   28.   80.   51.   32.   28.    9.   28.\n",
      " 32.   31.   41.   28.   20.   24.    2.   28.    0.75 48.   19.   56.\n",
      " 28.   23.   28.   18.   21.   28.   18.   24.   28.   32.   23.   58.\n",
      " 50.   40.   47.   36.   20.   32.   25.   28.   43.   28.   40.   31.\n",
      " 70.   31.   28.   18.   24.5  18.   43.   36.   28.   27.   20.   14.\n",
      " 60.   25.   14.   19.   18.   15.   31.    4.   28.   25.   60.   52.\n",
      " 44.   28.   49.   42.   18.   35.   18.   25.   26.   39.   45.   42.\n",
      " 22.   28.   24.   28.   48.   29.   52.   19.   38.   27.   28.   33.\n",
      "  6.   17.   34.   50.   27.   20.   30.   28.   25.   25.   29.   11.\n",
      " 28.   23.   23.   28.5  48.   35.   28.   28.   28.   36.   21.   24.\n",
      " 31.   70.   16.   30.   19.   31.    4.    6.   33.   23.   48.    0.67\n",
      " 28.   18.   34.   33.   28.   41.   20.   36.   16.   51.   28.   30.5\n",
      " 28.   32.   24.   48.   57.   28.   54.   18.   28.    5.   28.   43.\n",
      " 13.   17.   29.   28.   25.   25.   18.    8.    1.   46.   28.   16.\n",
      " 28.   28.   25.   39.   49.   31.   30.   30.   34.   31.   11.    0.42\n",
      " 27.   31.   39.   18.   39.   33.   26.   39.   35.    6.   30.5  28.\n",
      " 23.   31.   43.   10.   52.   27.   38.   27.    2.   28.   28.    1.\n",
      " 28.   62.   15.    0.83 28.   23.   18.   39.   21.   28.   32.   28.\n",
      " 20.   16.   30.   34.5  17.   42.   28.   35.   28.   28.    4.   74.\n",
      "  9.   16.   44.   18.   45.   51.   24.   28.   41.   21.   48.   28.\n",
      " 24.   42.   27.   31.   28.    4.   26.   47.   33.   47.   28.   15.\n",
      " 20.   19.   28.   56.   25.   33.   22.   28.   25.   39.   27.   19.\n",
      " 28.   26.   32.  ]\n"
     ]
    }
   ],
   "source": [
    "age_arr = x_df[\"Age\"].values\n",
    "fare_arr = x_df[\"Fare\"].values\n",
    "print(age_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d45431ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[22.   38.   26.   35.   35.   28.   54.    2.   27.   14.    4.   58.\n 20.   39.   14.   55.    2.   28.   31.   28.   35.   34.   15.   28.\n  8.   38.   28.   19.   28.   28.   40.   28.   28.   66.   28.   42.\n 28.   21.   18.   14.   40.   27.   28.    3.   19.   28.   28.   28.\n 28.   18.    7.   21.   49.   29.   65.   28.   21.   28.5   5.   11.\n 22.   38.   45.    4.   28.   28.   29.   19.   17.   26.   32.   16.\n 21.   26.   32.   25.   28.   28.    0.83 30.   22.   29.   28.   28.\n 17.   33.   16.   28.   23.   24.   29.   20.   46.   26.   59.   28.\n 71.   23.   34.   34.   28.   28.   21.   33.   37.   28.   21.   28.\n 38.   28.   47.   14.5  22.   20.   17.   21.   70.5  29.   24.    2.\n 21.   28.   32.5  32.5  54.   12.   28.   24.   28.   45.   33.   20.\n 47.   29.   25.   23.   19.   37.   16.   24.   28.   22.   24.   19.\n 18.   19.   27.    9.   36.5  42.   51.   22.   55.5  40.5  28.   51.\n 16.   30.   28.   28.   44.   40.   26.   17.    1.    9.   28.   45.\n 28.   28.   61.    4.    1.   21.   56.   18.   28.   50.   30.   36.\n 28.   28.    9.    1.    4.   28.   28.   45.   40.   36.   32.   19.\n 19.    3.   44.   58.   28.   42.   28.   24.   28.   28.   34.   45.5\n 18.    2.   32.   26.   16.   40.   24.   35.   22.   30.   28.   31.\n 27.   42.   32.   30.   16.   27.   51.   28.   38.   22.   19.   20.5\n 18.   28.   35.   29.   59.    5.   24.   28.   44.    8.   19.   33.\n 28.   28.   29.   22.   30.   44.   25.   24.   37.   54.   28.   29.\n 62.   30.   41.   29.   28.   30.   35.   50.   28.    3.   52.   40.\n 28.   36.   16.   25.   58.   35.   28.   25.   41.   37.   28.   63.\n 45.   28.    7.   35.   65.   28.   16.   19.   28.   33.   30.   22.\n 42.   22.   26.   19.   36.   24.   24.   28.   23.5   2.   28.   50.\n 28.   28.   19.   28.   28.    0.92 28.   17.   30.   30.   24.   18.\n 26.   28.   43.   26.   24.   54.   31.   40.   22.   27.   30.   22.\n 28.   36.   61.   36.   31.   16.   28.   45.5  38.   16.   28.   28.\n 29.   41.   45.   45.    2.   24.   28.   25.   36.   24.   40.   28.\n  3.   42.   23.   28.   15.   25.   28.   28.   22.   38.   28.   28.\n 40.   29.   45.   35.   28.   30.   60.   28.   28.   24.   25.   18.\n 19.   22.    3.   28.   22.   27.   20.   19.   42.    1.   32.   35.\n 28.   18.    1.   36.   28.   17.   36.   21.   28.   23.   24.   22.\n 31.   46.   23.   28.   39.   26.   21.   28.   20.   34.   51.    3.\n 21.   28.   28.   28.   33.   28.   44.   28.   34.   18.   30.   10.\n 28.   21.   29.   28.   18.   28.   28.   19.   28.   32.   28.   28.\n 42.   17.   50.   14.   21.   24.   64.   31.   45.   20.   25.   28.\n 28.    4.   13.   34.    5.   52.   36.   28.   30.   49.   28.   29.\n 65.   28.   50.   28.   48.   34.   47.   48.   28.   38.   28.   56.\n 28.    0.75 28.   38.   33.   23.   22.   28.   34.   29.   22.    2.\n  9.   28.   50.   63.   25.   28.   35.   58.   30.    9.   28.   21.\n 55.   71.   21.   28.   54.   28.   25.   24.   17.   21.   28.   37.\n 16.   18.   33.   28.   28.   26.   29.   28.   36.   54.   24.   47.\n 34.   28.   36.   32.   30.   22.   28.   44.   28.   40.5  50.   28.\n 39.   23.    2.   28.   17.   28.   30.    7.   45.   30.   28.   22.\n 36.    9.   11.   32.   50.   64.   19.   28.   33.    8.   17.   27.\n 28.   22.   22.   62.   48.   28.   39.   36.   28.   40.   28.   28.\n 28.   24.   19.   29.   28.   32.   62.   53.   36.   28.   16.   19.\n 34.   39.   28.   32.   25.   39.   54.   36.   28.   18.   47.   60.\n 22.   28.   35.   52.   47.   28.   37.   36.   28.   49.   28.   49.\n 24.   28.   28.   44.   35.   36.   30.   27.   22.   40.   39.   28.\n 28.   28.   35.   24.   34.   26.    4.   26.   27.   42.   20.   21.\n 21.   61.   57.   21.   26.   28.   80.   51.   32.   28.    9.   28.\n 32.   31.   41.   28.   20.   24.    2.   28.    0.75 48.   19.   56.\n 28.   23.   28.   18.   21.   28.   18.   24.   28.   32.   23.   58.\n 50.   40.   47.   36.   20.   32.   25.   28.   43.   28.   40.   31.\n 70.   31.   28.   18.   24.5  18.   43.   36.   28.   27.   20.   14.\n 60.   25.   14.   19.   18.   15.   31.    4.   28.   25.   60.   52.\n 44.   28.   49.   42.   18.   35.   18.   25.   26.   39.   45.   42.\n 22.   28.   24.   28.   48.   29.   52.   19.   38.   27.   28.   33.\n  6.   17.   34.   50.   27.   20.   30.   28.   25.   25.   29.   11.\n 28.   23.   23.   28.5  48.   35.   28.   28.   28.   36.   21.   24.\n 31.   70.   16.   30.   19.   31.    4.    6.   33.   23.   48.    0.67\n 28.   18.   34.   33.   28.   41.   20.   36.   16.   51.   28.   30.5\n 28.   32.   24.   48.   57.   28.   54.   18.   28.    5.   28.   43.\n 13.   17.   29.   28.   25.   25.   18.    8.    1.   46.   28.   16.\n 28.   28.   25.   39.   49.   31.   30.   30.   34.   31.   11.    0.42\n 27.   31.   39.   18.   39.   33.   26.   39.   35.    6.   30.5  28.\n 23.   31.   43.   10.   52.   27.   38.   27.    2.   28.   28.    1.\n 28.   62.   15.    0.83 28.   23.   18.   39.   21.   28.   32.   28.\n 20.   16.   30.   34.5  17.   42.   28.   35.   28.   28.    4.   74.\n  9.   16.   44.   18.   45.   51.   24.   28.   41.   21.   48.   28.\n 24.   42.   27.   31.   28.    4.   26.   47.   33.   47.   28.   15.\n 20.   19.   28.   56.   25.   33.   22.   28.   25.   39.   27.   19.\n 28.   26.   32.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      2\u001b[0m ss \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 3\u001b[0m age_arr \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mage_arr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    840\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 841\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[22.   38.   26.   35.   35.   28.   54.    2.   27.   14.    4.   58.\n 20.   39.   14.   55.    2.   28.   31.   28.   35.   34.   15.   28.\n  8.   38.   28.   19.   28.   28.   40.   28.   28.   66.   28.   42.\n 28.   21.   18.   14.   40.   27.   28.    3.   19.   28.   28.   28.\n 28.   18.    7.   21.   49.   29.   65.   28.   21.   28.5   5.   11.\n 22.   38.   45.    4.   28.   28.   29.   19.   17.   26.   32.   16.\n 21.   26.   32.   25.   28.   28.    0.83 30.   22.   29.   28.   28.\n 17.   33.   16.   28.   23.   24.   29.   20.   46.   26.   59.   28.\n 71.   23.   34.   34.   28.   28.   21.   33.   37.   28.   21.   28.\n 38.   28.   47.   14.5  22.   20.   17.   21.   70.5  29.   24.    2.\n 21.   28.   32.5  32.5  54.   12.   28.   24.   28.   45.   33.   20.\n 47.   29.   25.   23.   19.   37.   16.   24.   28.   22.   24.   19.\n 18.   19.   27.    9.   36.5  42.   51.   22.   55.5  40.5  28.   51.\n 16.   30.   28.   28.   44.   40.   26.   17.    1.    9.   28.   45.\n 28.   28.   61.    4.    1.   21.   56.   18.   28.   50.   30.   36.\n 28.   28.    9.    1.    4.   28.   28.   45.   40.   36.   32.   19.\n 19.    3.   44.   58.   28.   42.   28.   24.   28.   28.   34.   45.5\n 18.    2.   32.   26.   16.   40.   24.   35.   22.   30.   28.   31.\n 27.   42.   32.   30.   16.   27.   51.   28.   38.   22.   19.   20.5\n 18.   28.   35.   29.   59.    5.   24.   28.   44.    8.   19.   33.\n 28.   28.   29.   22.   30.   44.   25.   24.   37.   54.   28.   29.\n 62.   30.   41.   29.   28.   30.   35.   50.   28.    3.   52.   40.\n 28.   36.   16.   25.   58.   35.   28.   25.   41.   37.   28.   63.\n 45.   28.    7.   35.   65.   28.   16.   19.   28.   33.   30.   22.\n 42.   22.   26.   19.   36.   24.   24.   28.   23.5   2.   28.   50.\n 28.   28.   19.   28.   28.    0.92 28.   17.   30.   30.   24.   18.\n 26.   28.   43.   26.   24.   54.   31.   40.   22.   27.   30.   22.\n 28.   36.   61.   36.   31.   16.   28.   45.5  38.   16.   28.   28.\n 29.   41.   45.   45.    2.   24.   28.   25.   36.   24.   40.   28.\n  3.   42.   23.   28.   15.   25.   28.   28.   22.   38.   28.   28.\n 40.   29.   45.   35.   28.   30.   60.   28.   28.   24.   25.   18.\n 19.   22.    3.   28.   22.   27.   20.   19.   42.    1.   32.   35.\n 28.   18.    1.   36.   28.   17.   36.   21.   28.   23.   24.   22.\n 31.   46.   23.   28.   39.   26.   21.   28.   20.   34.   51.    3.\n 21.   28.   28.   28.   33.   28.   44.   28.   34.   18.   30.   10.\n 28.   21.   29.   28.   18.   28.   28.   19.   28.   32.   28.   28.\n 42.   17.   50.   14.   21.   24.   64.   31.   45.   20.   25.   28.\n 28.    4.   13.   34.    5.   52.   36.   28.   30.   49.   28.   29.\n 65.   28.   50.   28.   48.   34.   47.   48.   28.   38.   28.   56.\n 28.    0.75 28.   38.   33.   23.   22.   28.   34.   29.   22.    2.\n  9.   28.   50.   63.   25.   28.   35.   58.   30.    9.   28.   21.\n 55.   71.   21.   28.   54.   28.   25.   24.   17.   21.   28.   37.\n 16.   18.   33.   28.   28.   26.   29.   28.   36.   54.   24.   47.\n 34.   28.   36.   32.   30.   22.   28.   44.   28.   40.5  50.   28.\n 39.   23.    2.   28.   17.   28.   30.    7.   45.   30.   28.   22.\n 36.    9.   11.   32.   50.   64.   19.   28.   33.    8.   17.   27.\n 28.   22.   22.   62.   48.   28.   39.   36.   28.   40.   28.   28.\n 28.   24.   19.   29.   28.   32.   62.   53.   36.   28.   16.   19.\n 34.   39.   28.   32.   25.   39.   54.   36.   28.   18.   47.   60.\n 22.   28.   35.   52.   47.   28.   37.   36.   28.   49.   28.   49.\n 24.   28.   28.   44.   35.   36.   30.   27.   22.   40.   39.   28.\n 28.   28.   35.   24.   34.   26.    4.   26.   27.   42.   20.   21.\n 21.   61.   57.   21.   26.   28.   80.   51.   32.   28.    9.   28.\n 32.   31.   41.   28.   20.   24.    2.   28.    0.75 48.   19.   56.\n 28.   23.   28.   18.   21.   28.   18.   24.   28.   32.   23.   58.\n 50.   40.   47.   36.   20.   32.   25.   28.   43.   28.   40.   31.\n 70.   31.   28.   18.   24.5  18.   43.   36.   28.   27.   20.   14.\n 60.   25.   14.   19.   18.   15.   31.    4.   28.   25.   60.   52.\n 44.   28.   49.   42.   18.   35.   18.   25.   26.   39.   45.   42.\n 22.   28.   24.   28.   48.   29.   52.   19.   38.   27.   28.   33.\n  6.   17.   34.   50.   27.   20.   30.   28.   25.   25.   29.   11.\n 28.   23.   23.   28.5  48.   35.   28.   28.   28.   36.   21.   24.\n 31.   70.   16.   30.   19.   31.    4.    6.   33.   23.   48.    0.67\n 28.   18.   34.   33.   28.   41.   20.   36.   16.   51.   28.   30.5\n 28.   32.   24.   48.   57.   28.   54.   18.   28.    5.   28.   43.\n 13.   17.   29.   28.   25.   25.   18.    8.    1.   46.   28.   16.\n 28.   28.   25.   39.   49.   31.   30.   30.   34.   31.   11.    0.42\n 27.   31.   39.   18.   39.   33.   26.   39.   35.    6.   30.5  28.\n 23.   31.   43.   10.   52.   27.   38.   27.    2.   28.   28.    1.\n 28.   62.   15.    0.83 28.   23.   18.   39.   21.   28.   32.   28.\n 20.   16.   30.   34.5  17.   42.   28.   35.   28.   28.    4.   74.\n  9.   16.   44.   18.   45.   51.   24.   28.   41.   21.   48.   28.\n 24.   42.   27.   31.   28.    4.   26.   47.   33.   47.   28.   15.\n 20.   19.   28.   56.   25.   33.   22.   28.   25.   39.   27.   19.\n 28.   26.   32.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "age_arr = ss.fit(age_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64733489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0909334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of 477    0\n",
       "17     1\n",
       "159    0\n",
       "107    1\n",
       "332    0\n",
       "      ..\n",
       "13     0\n",
       "171    0\n",
       "355    0\n",
       "164    0\n",
       "440    1\n",
       "Name: Survived, Length: 712, dtype: int64>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "468f55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f16f35db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8324022346368715"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1ff67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
